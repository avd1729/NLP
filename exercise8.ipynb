{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.Categorizing and Tagging words(Brown Corpus)\n",
    "\ta)Representing Tagged tokens\n",
    "\tb)Reading TAgges corpora\n",
    "\tc)Focnstruct a frequency distribution from the tag parts of the bigrams\n",
    "\td)find words ivolving particular sequences of tags and words(in this case\"<Verb> tp <Verb>\").\n",
    "\te)Searching for three-Word phrases using POS Tagging.\n",
    "\tf)Impelement Automatic Tagging\n",
    "\tg)Implement Regular Expression Tagger\n",
    "\th)Implement Unigram Tagging\n",
    "\ti)Implement N-Gram Tagging\n",
    "\tj)Implement Transformation Based Tagging\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "Tags: ['DT', 'JJ', 'JJ', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN']\n",
      "Tag Frequency: <FreqDist with 5 samples and 9 outcomes>\n",
      "Most Common Tag: ('JJ', 3)\n",
      "Tags for 'fox': ['NN']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample tagged tokens\n",
    "tagged_tokens = [(\"The\", \"DT\"), (\"quick\", \"JJ\"), (\"brown\", \"JJ\"), (\"fox\", \"NN\"), (\"jumps\", \"VBZ\"), (\"over\", \"IN\"), (\"the\", \"DT\"), (\"lazy\", \"JJ\"), (\"dog\", \"NN\")]\n",
    "\n",
    "# Access individual tokens and tags\n",
    "tokens = [token for token, tag in tagged_tokens]\n",
    "tags = [tag for token, tag in tagged_tokens]\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Tags:\", tags)\n",
    "\n",
    "# Get frequency distribution of tags\n",
    "tag_freq = nltk.FreqDist(tags)\n",
    "print(\"Tag Frequency:\", tag_freq)\n",
    "\n",
    "# Find the most common tag\n",
    "most_common_tag = tag_freq.most_common(1)[0]\n",
    "print(\"Most Common Tag:\", most_common_tag)\n",
    "\n",
    "# Find tags associated with a specific word (e.g., \"fox\")\n",
    "word_to_find = \"fox\"\n",
    "tags_for_word = [tag for token, tag in tagged_tokens if token.lower() == word_to_find.lower()]\n",
    "print(f\"Tags for '{word_to_find}': {tags_for_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in the Brown Corpus: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "[('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]\n",
      "[('The', 'AT'), ('September-October', 'NP'), ('term', 'NN'), ('jury', 'NN'), ('had', 'HVD'), ('been', 'BEN'), ('charged', 'VBN'), ('by', 'IN'), ('Fulton', 'NP-TL'), ('Superior', 'JJ-TL'), ('Court', 'NN-TL'), ('Judge', 'NN-TL'), ('Durwood', 'NP'), ('Pye', 'NP'), ('to', 'TO'), ('investigate', 'VB'), ('reports', 'NNS'), ('of', 'IN'), ('possible', 'JJ'), ('``', '``'), ('irregularities', 'NNS'), (\"''\", \"''\"), ('in', 'IN'), ('the', 'AT'), ('hard-fought', 'JJ'), ('primary', 'NN'), ('which', 'WDT'), ('was', 'BEDZ'), ('won', 'VBN'), ('by', 'IN'), ('Mayor-nominate', 'NN-TL'), ('Ivan', 'NP'), ('Allen', 'NP'), ('Jr.', 'NP'), ('.', '.')]\n",
      "Word: The, Tag: AT\n",
      "Word: Fulton, Tag: NP-TL\n",
      "Word: County, Tag: NN-TL\n",
      "Word: Grand, Tag: JJ-TL\n",
      "Word: Jury, Tag: NN-TL\n",
      "Word: said, Tag: VBD\n",
      "Word: Friday, Tag: NR\n",
      "Word: an, Tag: AT\n",
      "Word: investigation, Tag: NN\n",
      "Word: of, Tag: IN\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# List categories in the Brown Corpus\n",
    "categories = brown.categories()\n",
    "print(\"Categories in the Brown Corpus:\", categories)\n",
    "\n",
    "# Access tagged sentences in a specific category (e.g., \"news\")\n",
    "tagged_sentences = brown.tagged_sents(categories=\"news\")\n",
    "\n",
    "# Print the first few tagged sentences\n",
    "for sentence in tagged_sentences[:3]:\n",
    "    print(sentence)\n",
    "\n",
    "# Access tagged words in a specific category (e.g., \"news\")\n",
    "tagged_words = brown.tagged_words(categories=\"news\")\n",
    "\n",
    "# Print the first few tagged words\n",
    "for word, tag in tagged_words[:10]:\n",
    "    print(f\"Word: {word}, Tag: {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: ('the', 'AT'), Frequency: 5558\n",
      "Tag: (',', ','), Frequency: 5133\n",
      "Tag: ('.', '.'), Frequency: 4012\n",
      "Tag: ('of', 'IN'), Frequency: 2716\n",
      "Tag: ('and', 'CC'), Frequency: 2115\n",
      "Tag: ('a', 'AT'), Frequency: 1988\n",
      "Tag: ('in', 'IN'), Frequency: 1828\n",
      "Tag: ('to', 'TO'), Frequency: 1222\n",
      "Tag: ('for', 'IN'), Frequency: 905\n",
      "Tag: ('to', 'IN'), Frequency: 880\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "# Access tagged sentences in a specific category (e.g., \"news\")\n",
    "tagged_sentences = brown.tagged_sents(categories=\"news\")\n",
    "\n",
    "# Create bigrams from the tagged sentences\n",
    "tag_bigrams = list(bigrams(tag for sentence in tagged_sentences for tag in sentence))\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Extract the tag parts (second element of each bigram)\n",
    "tags = [tag for _, tag in tag_bigrams]\n",
    "\n",
    "# Create a frequency distribution from the tag parts\n",
    "tag_freq_dist = FreqDist(tags)\n",
    "\n",
    "# Print the most common tags and their frequencies\n",
    "most_common_tags = tag_freq_dist.most_common(10)  # Change the number as needed\n",
    "for tag, frequency in most_common_tags:\n",
    "    print(f\"Tag: {tag}, Frequency: {frequency}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words involving the sequence '<Verb> to <Verb>': set()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Define the desired sequence of tags and words\n",
    "desired_sequence = [(\"VB\", \"to\", \"VB\")]\n",
    "\n",
    "# Function to find words involving the desired sequence\n",
    "def find_words_with_sequence(tagged_text, sequence):\n",
    "    found_words = set()\n",
    "    for sentence in tagged_text:\n",
    "        # Create bigrams from the tagged sentence\n",
    "        bigrams = list(ngrams(sentence, 3))\n",
    "        for bigram in bigrams:\n",
    "            if bigram == sequence:\n",
    "                found_words.add(bigram[1])  # Add the middle word to the set\n",
    "    return found_words\n",
    "\n",
    "# Tag and tokenize sentences in the Brown Corpus\n",
    "tagged_sentences = [pos_tag(sentence) for sentence in brown.sents()]\n",
    "\n",
    "# Find words involving the desired sequence\n",
    "words_with_sequence = find_words_with_sequence(tagged_sentences, desired_sequence)\n",
    "\n",
    "# Print the words found\n",
    "print(\"Words involving the sequence '<Verb> to <Verb>':\", words_with_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases involving the sequence '<Noun> <Preposition> <Determiner>': set()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Define the desired sequence of tags\n",
    "desired_sequence = [(\"NN\", \"IN\", \"DT\")]\n",
    "\n",
    "# Function to find phrases involving the desired sequence\n",
    "def find_phrases_with_sequence(tagged_text, sequence):\n",
    "    found_phrases = set()\n",
    "    for sentence in tagged_text:\n",
    "        # Create trigrams from the tagged sentence\n",
    "        trigrams = list(ngrams(sentence, 3))\n",
    "        for trigram in trigrams:\n",
    "            if trigram == sequence:\n",
    "                phrase = \" \".join(word for word, _ in trigram)\n",
    "                found_phrases.add(phrase)\n",
    "    return found_phrases\n",
    "\n",
    "# Tag and tokenize sentences in the Brown Corpus\n",
    "tagged_sentences = [pos_tag(sentence) for sentence in brown.sents()]\n",
    "\n",
    "# Find phrases involving the desired sequence\n",
    "phrases_with_sequence = find_phrases_with_sequence(tagged_sentences, desired_sequence)\n",
    "\n",
    "# Print the phrases found\n",
    "print(\"Phrases involving the sequence '<Noun> <Preposition> <Determiner>':\", phrases_with_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('leading', 'VERB'), ('platform', 'NOUN'), ('for', 'ADP'), ('building', 'VERB'), ('Python', 'NOUN'), ('programs', 'NOUN'), ('to', 'PRT'), ('work', 'VERB'), ('with', 'ADP'), ('human', 'ADJ'), ('language', 'NOUN'), ('data', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "# Sample text for tagging\n",
    "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging using the pre-trained tagger\n",
    "tagged_words = pos_tag(words, tagset=\"universal\")\n",
    "\n",
    "# View the tagged results\n",
    "print(tagged_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', None), ('want', None), ('to', None), ('buy', None), ('an', None), ('apple', 'NN'), ('and', None), ('run', 'VB'), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "# Define regular expression patterns\n",
    "patterns = [\n",
    "    (r'^[Aa]pple$', 'NN'),        # Matches \"Apple\" or \"apple\" as a noun\n",
    "    (r'^[Bb]anana$', 'NN'),       # Matches \"Banana\" or \"banana\" as a noun\n",
    "    (r'^[Cc]ar$', 'NN'),          # Matches \"Car\" or \"car\" as a noun\n",
    "    (r'^[Rr]un$', 'VB'),          # Matches \"Run\" or \"run\" as a verb\n",
    "    (r'^[Jj]ump$', 'VB'),         # Matches \"Jump\" or \"jump\" as a verb\n",
    "    # Add more patterns as needed\n",
    "]\n",
    "\n",
    "# Create the regular expression tagger\n",
    "regexp_tagger = RegexpTagger(patterns)\n",
    "\n",
    "# Test the regular expression tagger on a sentence\n",
    "sentence = \"I want to buy an apple and run.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Tag the words using the regular expression tagger\n",
    "tagged_words = regexp_tagger.tag(words)\n",
    "\n",
    "# Print the tagged words\n",
    "print(tagged_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', None), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', None), ('for', 'IN'), ('building', 'NN'), ('Python', None), ('programs', 'NNS'), ('to', 'TO'), ('work', 'NN'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# Sample text for tagging\n",
    "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Train the UnigramTagger on the Penn Treebank corpus\n",
    "unigram_tagger = UnigramTagger(treebank.tagged_sents())\n",
    "\n",
    "# Tag the words using the UnigramTagger\n",
    "tagged_words = unigram_tagger.tag(words)\n",
    "\n",
    "# Print the tagged words\n",
    "print(tagged_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', None), ('is', None), ('a', None), ('leading', None), ('platform', None), ('for', None), ('building', None), ('Python', None), ('programs', None), ('to', None), ('work', None), ('with', None), ('human', None), ('language', None), ('data', None), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import NgramTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# Sample text for tagging\n",
    "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Define the N-gram order (e.g., 2 for bigrams)\n",
    "n = 2\n",
    "\n",
    "# Train the NgramTagger on the Penn Treebank corpus\n",
    "ngram_tagger = NgramTagger(n, treebank.tagged_sents())\n",
    "\n",
    "# Tag the words using the NgramTagger\n",
    "tagged_words = ngram_tagger.tag(words)\n",
    "\n",
    "# Print the tagged words\n",
    "print(tagged_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
